{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voting classifiers#hard\n",
    "#one way to create ensemble is to combine different training alogorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "moons = make_moons()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = moons[0]\n",
    "y = moons[1]\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "rc = RandomForestClassifier()\n",
    "sv = SVC()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(estimators = [(\"lr1\",lr),(\"rc1\",rc),(\"sv1\",sv)],voting = \"hard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.8\n",
      "RandomForestClassifier 0.9\n",
      "SVC 1.0\n",
      "VotingClassifier 0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (lr,rc,sv,voting_clf):\n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(clf.__class__.__name__,accuracy_score(y_test,y_pred))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another way is to combine model osf same training algorithm but on different datasets\n",
    "#bagging and pasting #sampling with replacement and without replacement\n",
    "#softvotingclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9625"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for bagging bootstrap = true,false for pasting  \n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(),n_estimators = 500,max_samples =1.0,bootstrap=True,n_jobs = -1 ,oob_score = True)\n",
    "bag_clf.fit(x_train,y_train)\n",
    "bag_clf.oob_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(x_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.875     , 0.125     ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00518135, 0.99481865],\n",
       "       [0.21782178, 0.78217822],\n",
       "       [0.01612903, 0.98387097],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99      , 0.01      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0255102 , 0.9744898 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.54705882, 0.45294118],\n",
       "       [0.46666667, 0.53333333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.16080402, 0.83919598],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.53888889, 0.46111111],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.95287958, 0.04712042],\n",
       "       [0.10695187, 0.89304813],\n",
       "       [0.24456522, 0.75543478],\n",
       "       [0.02604167, 0.97395833],\n",
       "       [0.79569892, 0.20430108],\n",
       "       [0.44444444, 0.55555556],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.92528736, 0.07471264],\n",
       "       [0.98795181, 0.01204819],\n",
       "       [1.        , 0.        ],\n",
       "       [0.69375   , 0.30625   ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.015625  , 0.984375  ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.43406593, 0.56593407],\n",
       "       [0.12834225, 0.87165775],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.48333333, 0.51666667],\n",
       "       [0.01754386, 0.98245614],\n",
       "       [0.245     , 0.755     ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98342541, 0.01657459],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96195652, 0.03804348],\n",
       "       [1.        , 0.        ],\n",
       "       [0.92349727, 0.07650273],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00917431, 0.99082569],\n",
       "       [0.73184358, 0.26815642],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.91099476, 0.08900524],\n",
       "       [0.        , 1.        ],\n",
       "       [0.95454545, 0.04545455],\n",
       "       [1.        , 0.        ],\n",
       "       [0.56043956, 0.43956044],\n",
       "       [0.37096774, 0.62903226],\n",
       "       [0.        , 1.        ],\n",
       "       [0.91304348, 0.08695652],\n",
       "       [0.96470588, 0.03529412],\n",
       "       [1.        , 0.        ],\n",
       "       [0.13043478, 0.86956522],\n",
       "       [0.18604651, 0.81395349],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to see the probability for each class for the estimator having predict_prob() method\n",
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead of using baggingclassifier with decisiontrees we can directly use randomforestclassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500,max_leaf_nodes = 16,n_jobs = -1)\n",
    "rnd_clf.fit(x_train,y_train)\n",
    "y_pred = rnd_clf.predict(x_test)\n",
    "#equivalent bagging classifier\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(splitter = \"Random\",max_leaf_nodes = 16),\n",
    "                            n_estimators = 500,max_samples = 1.0,bootstrap= True,n_jobs = -1)\n",
    "                           \n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.10173304101218593\n",
      "sepal width (cm) 0.025523897112434624\n",
      "petal length (cm) 0.43926772122438373\n",
      "petal width (cm) 0.43347534065099574\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500,n_jobs = -1)\n",
    "rnd_clf.fit(iris[\"data\"],iris[\"target\"])\n",
    "for name,score in zip(iris[\"feature_names\"],rnd_clf.feature_importances_):\n",
    "    print(name,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 1),n_estimators= 200,algorithm =\"SAMME.R\",learning_rate = 0.5)\n",
    "ada_clf.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without class gradient boost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg.fit(x,y)\n",
    "y2= y -  tree_reg.predict(x)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg2.fit(x,y2)\n",
    "y3= y -  tree_reg2.predict(x)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg3.fit(x,y3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(x) for tree in (tree_reg,tree_reg2,tree_reg3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt= GradientBoostingRegressor(max_depth = 2,n_estimators = 3,learning_rate=1.0)\n",
    "gbrt.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=99)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_val,y_train,y_val= train_test_split(x,y)\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2,n_estimators = 100)\n",
    "gbrt.fit(x_train,y_train)\n",
    "errors = [mean_squared_error(y_pred,y_val) for y_pred in gbrt.staged_predict(x_val)]\n",
    "best_n_estimators = np.argmin(errors)+1\n",
    "gbrt_best= GradientBoostingRegressor(max_depth = 2,n_estimators = best_n_estimators)\n",
    "gbrt_best.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
